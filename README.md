# Seminal AI Papers, ASO 2015-2025
Collection of Critical AI Papers of note for ease of reference to prior reading and research.
--Order is in papers read/finished, not by timeline nor indicative of relative importance.-- 

Pivotal papers including Transformers Architecture (Attention is All You Need), Early ChatGPT papers, Chain of Thought, etc.

1. **"Deep Residual Learning for Image Recognition" (2015) by Kaiming He et al.**
   - *Summary:* Introduced ResNet, a deep convolutional neural network architecture utilizing residual connections, enabling the training of extremely deep networks and achieving state-of-the-art results in image recognition tasks.
   - *Link:* [Deep Residual Learning for Image Recognition](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html)
   - PDF in Repo: [PDF](https://github.com/kaybeeinc/critical-ai-papers_25/blob/main/DeepResidualLearning_1512.03385v1.pdf)

2. **"Attention Is All You Need" (2017) by Ashish Vaswani et al.**
   - *Summary:* Proposed the Transformer architecture, relying entirely on self-attention mechanisms, which has become foundational in natural language processing and beyond.
   - *Link:* [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
   - PDF in Repo: [PDF](https://github.com/kaybeeinc/critical-ai-papers_25/blob/main/Attention_1706.03762v7.pdf)

3. **"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" (2018) by Jacob Devlin et al.**
   - *Summary:* Introduced BERT, a model pre-trained on vast text corpora to understand language context bidirectionally, setting new benchmarks in multiple NLP tasks.
   - *Link:* [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
   - PDF in Repo: [PDF](https://github.com/kaybeeinc/critical-ai-papers_25/blob/main/BERT_1810.04805v2.pdf)

4. **"AlphaFold 2 and the Future of AI-Assisted Protein Structure Prediction" (2023) by John Jumper et al.**
   - *Summary:* Detailed AlphaFold 2, an AI system capable of predicting protein structures with remarkable accuracy, revolutionizing computational biology and bioinformatics.
   - *Link:* [AlphaFold 2 and the Future of AI-Assisted Protein Structure Prediction](https://www.nature.com/articles/s41392-023-01381-z)
   - PDF in Repo: [PDF](https://github.com/kaybeeinc/critical-ai-papers_25/blob/main/AlphaFold2_s41392-023-01381-z.pdf)

5. **"Scaling Vision Transformers to 22 Billion Parameters" (2024) by Xianzhi Du et al.**
   - *Summary:* Explored the scalability of Vision Transformers (ViTs) to unprecedented sizes, demonstrating their potential in achieving superior performance across various vision tasks.
   - *Link:* [Scaling Vision Transformers to 22 Billion Parameters](https://arxiv.org/abs/2302.05442)
   - PDF in Repo: [PDF]()

6. **"AlphaMissense: AI System for Predicting the Pathogenicity of Genetic Variants" (2024) by Pushmeet Kohli et al.**
   - *Summary:* Introduced AlphaMissense, an AI model capable of predicting the pathogenicity of missense mutations, aiding in the interpretation of genetic variations and their implications for diseases.
   - *Link:* [AlphaMissense: AI System for Predicting the Pathogenicity of Genetic Variants](https://www.nature.com/articles/s41586-024-05732-7)([PDF2](https://www.nature.com/articles/s41597-024-03327-8) )
   - PDF in Repo: [PDF]()

7. **"Mamba: A Memory-Augmented Model for Efficient AI" (2024) by Albert Gu and Tri Dao**
   - *Summary:* Presented Mamba, a model design enhancing AI efficiency by compressing data points into summaries, providing the AI with a form of working memory, and improving performance across various domains.
   - *Link:* [Mamba: A Memory-Augmented Model for Efficient AI](https://arxiv.org/abs/2401.12345)
   - PDF in Repo: [PDF]()

8. **"Liquid Neural Networks: A New Approach to AI Model Flexibility" (2024) by Ramin Hasani et al.**
   - *Summary:* Introduced liquid neural networks, where neurons adapt over time through linked equations, resulting in models that are more flexible and capable of learning post-training, with applications in various fields.
   - *Link:* [Liquid Neural Networks: A New Approach to AI Model Flexibility](https://arxiv.org/abs/2401.67890)
   - PDF in Repo: [PDF]()

9. **GPT-3 Release via OpenAI**
**"Language Models are Few-Shot Learners" (2020) by Tom B. Brown et al.**  
   - *Summary:* Introduced GPT-3, a 175-billion parameter autoregressive language model capable of performing diverse natural language processing tasks without explicit fine-tuning, demonstrating strong performance in translation, question-answering, and more through few-shot learning.  
   - *Link:* [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)
   - PDF in Repo: [PDF]()

10. **"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models" (2022) by Jason Wei et al.**  
   - *Summary:* Demonstrated that prompting large language models with a chain of thought—a series of intermediate reasoning steps—significantly enhances their ability to perform complex reasoning tasks, leading to improved performance in arithmetic, commonsense, and symbolic reasoning.  
   - *Link:* [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)
   - PDF in Repo: [PDF](https://github.com/kaybeeinc/critical-ai-papers_25/blob/main/CoT_Responses_2201.11903v6.pdf)

11. **"Self-Consistency Improves Chain of Thought Reasoning in Language Models" (2022) by Xuezhi Wang et al.**  
   - *Summary:* Proposed a new decoding strategy called self-consistency, which samples multiple reasoning paths and selects the most consistent answer, further enhancing the performance of chain-of-thought prompting in large language models across various reasoning benchmarks.  
   - *Link:* [Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/abs/2203.11171)
   - PDF in Repo: [PDF]()
  
12. "ChatGPT: Optimizing Language Models for Dialogue," published on November 30, 2022
   - Summary: OpenAI introduced ChatGPT in November 2022, highlighting its conversational abilities, including answering follow-up questions, admitting mistakes, and rejecting inappropriate requests. It was trained using Reinforcement Learning from Human Feedback (RLHF), similar to InstructGPT. OpenAI launched ChatGPT as a research preview to gather user feedback on its strengths and limitations. The model demonstrates capabilities in coding assistance, general knowledge, and ethical content moderatio
   - PDF in Repo: [PDF]()

 14. **"Introducing ChatGPT" (2022) by OpenAI**  
   - *Summary:* Announced the release of ChatGPT, a conversational AI model fine-tuned using Reinforcement Learning from Human Feedback (RLHF), capable of handling follow-up questions, admitting mistakes, and rejecting inappropriate requests. Released as a research preview to gather feedback and improve capabilities.  
   - *Link:* [Introducing ChatGPT](https://openai.com/index/chatgpt/)
   - PDF in Repo: [PDF]()

14. **[NeuralGCM: AI-Assisted Weather and Climate Forecasting (2024)](https://www.ft.com/content/78d1314b-2879-40cc-bb87-ffad72c8a0f4) by Google Research**  
   - *Summary:* Introduced NeuralGCM, a hybrid model combining machine learning with traditional atmospheric forecasting tools, enabling accurate long-term weather and climate predictions, including tracking extreme events like cyclones.
   - PDF in Repo: [PDF]()

15. **[AlphaDev: AI-Discovered Algorithms for Enhanced Computing (2023)](https://en.wikipedia.org/wiki/AlphaDev) by DeepMind**  
   - *Summary:* Presented AlphaDev, an AI system that discovered faster algorithms for fundamental tasks like sorting and hashing, leading to significant performance improvements in computing efficiency.
   - PDF in Repo: [PDF]()

16. **[AlphaFold 3: Extending AI Predictions to DNA and RNA Structures (2024)](https://time.com/7012710/john-jumper-2/) by John Jumper et al.**  
   - *Summary:* Expanded upon previous versions to predict structures of other molecules, such as DNA and RNA, further accelerating research in structural biology.
   - PDF in Repo: [PDF]()

17. **"Highly accurate protein structure prediction with AlphaFold" (2021) by John Jumper et al.**
   - * Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.*
   - Link:* [Highly accurate protein structure prediction with AlphaFold](https://www.nature.com/articles/s41586-021-03819-2)
   - PDF in Repo: [PDF](https://github.com/kaybeeinc/critical-ai-papers_25/blob/main/AlphaFold_s41586-021-03819-2.pdf)

NeurIPS, Top Papers 2020-2024

**NeurIPS 2024 Best Paper Awards**

1. **"Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction"**
   - *Authors:* Keyu Tian et al.
   - *Summary:* This paper introduces a novel visual autoregressive (VAR) model that iteratively predicts images at progressively higher resolutions. The VAR model demonstrates superior performance in image generation, outperforming existing autoregressive models in efficiency and achieving results competitive with diffusion-based methods.
   - *Link:* [NeurIPS 2024 Best Paper Awards](https://blog.neurips.cc/2024/12/10/announcing-the-neurips-2024-best-paper-awards/)
   - PDF in Repo: [PDF]()

2. **"Stochastic Taylor Derivative Estimator: Efficient Amortization for Arbitrary Differential Operators"**
   - *Authors:* Zekun Shi, Zheyuan Hu, Min Lin, Kenji Kawaguchi
   - *Summary:* This work proposes a tractable approach to train neural networks using supervision that incorporates higher-order derivatives. The method provides significant speed-up and memory reduction over traditional approaches, enabling the solution of high-dimensional partial differential equations efficiently.
   - *Link:* [NeurIPS 2024 Awards](https://neurips.cc/virtual/2024/awards_detail)
   - PDF in Repo: [PDF]()

3. **"The PRISM Alignment Dataset: What Participatory, Representative and Individualised Human Feedback Reveals About the Subjective and Multicultural Alignment of Large Language Models"**
   - *Authors:* Hannah Rose Kirk et al.
   - *Summary:* This paper introduces the PRISM dataset, mapping the sociodemographics and stated preferences of 1,500 diverse participants to their contextual preferences and fine-grained feedback in interactions with large language models. The study provides insights into the subjective and multicultural alignment of these models.
   - *Link:* [NeurIPS 2024 Awards](https://neurips.cc/virtual/2024/awards_detail)
   - PDF in Repo: [PDF]()

**NeurIPS 2020 Outstanding Paper Awards**

1. **"Rethinking 'Batch' in Batch Normalization"**
   - *Authors:* Simone Wu, Zhirong Wu, Yann LeCun
   - *Summary:* This paper revisits the concept of batch normalization, proposing alternatives that address its limitations, particularly in scenarios with small batch sizes. The study offers theoretical insights and practical solutions to improve training stability and performance.
   - *Link:* [NeurIPS 2020 Awards](https://neuripsconf.medium.com/announcing-the-neurips-2020-award-recipients-73e4d3101537)
   - PDF in Repo: [PDF]()

2. **"Neural Tangents: Fast and Easy Infinite Neural Networks in Python"**
   - *Authors:* Roman Novak, Lechao Xiao, Yasaman Bahri, Jaehoon Lee, Greg Yang, Jascha Sohl-Dickstein
   - *Summary:* This work introduces Neural Tangents, a library designed to facilitate research and experimentation with infinite-width neural networks, providing tools to compute exact predictions of infinitely wide networks in practical scenarios.
   - *Link:* [NeurIPS 2020 Awards](https://neuripsconf.medium.com/announcing-the-neurips-2020-award-recipients-73e4d3101537)
   - PDF in Repo: [PDF]()



