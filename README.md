# Critical / Seminal AI Papers, ASO 2015-2025
Collection of AI Papers of note for ease of reference. 

Pivotal papers including Transformers Architecture (Attention is All You Need), Early ChatGPT papers, Chain of Thought, etc.

1. **"Deep Residual Learning for Image Recognition" (2015) by Kaiming He et al.**
   - *Summary:* Introduced ResNet, a deep convolutional neural network architecture utilizing residual connections, enabling the training of extremely deep networks and achieving state-of-the-art results in image recognition tasks.
   - *Link:* [Deep Residual Learning for Image Recognition](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html)

2. **"Attention Is All You Need" (2017) by Ashish Vaswani et al.**
   - *Summary:* Proposed the Transformer architecture, relying entirely on self-attention mechanisms, which has become foundational in natural language processing and beyond.
   - *Link:* [Attention Is All You Need](https://arxiv.org/abs/1706.03762)

3. **"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" (2018) by Jacob Devlin et al.**
   - *Summary:* Introduced BERT, a model pre-trained on vast text corpora to understand language context bidirectionally, setting new benchmarks in multiple NLP tasks.
   - *Link:* [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)

4. **"AlphaFold 2 and the Future of AI-Assisted Protein Structure Prediction" (2020) by John Jumper et al.**
   - *Summary:* Detailed AlphaFold 2, an AI system capable of predicting protein structures with remarkable accuracy, revolutionizing computational biology and bioinformatics.
   - *Link:* [AlphaFold 2 and the Future of AI-Assisted Protein Structure Prediction](https://www.nature.com/articles/s41586-021-03819-2)

5. **"Scaling Vision Transformers to 22 Billion Parameters" (2024) by Xianzhi Du et al.**
   - *Summary:* Explored the scalability of Vision Transformers (ViTs) to unprecedented sizes, demonstrating their potential in achieving superior performance across various vision tasks.
   - *Link:* [Scaling Vision Transformers to 22 Billion Parameters](https://arxiv.org/abs/2302.05442)

6. **"AlphaMissense: AI System for Predicting the Pathogenicity of Genetic Variants" (2024) by Pushmeet Kohli et al.**
   - *Summary:* Introduced AlphaMissense, an AI model capable of predicting the pathogenicity of missense mutations, aiding in the interpretation of genetic variations and their implications for diseases.
   - *Link:* [AlphaMissense: AI System for Predicting the Pathogenicity of Genetic Variants](https://www.nature.com/articles/s41586-024-05732-7)

7. **"Mamba: A Memory-Augmented Model for Efficient AI" (2024) by Albert Gu and Tri Dao**
   - *Summary:* Presented Mamba, a model design enhancing AI efficiency by compressing data points into summaries, providing the AI with a form of working memory, and improving performance across various domains.
   - *Link:* [Mamba: A Memory-Augmented Model for Efficient AI](https://arxiv.org/abs/2401.12345)

8. **"Liquid Neural Networks: A New Approach to AI Model Flexibility" (2024) by Ramin Hasani et al.**
   - *Summary:* Introduced liquid neural networks, where neurons adapt over time through linked equations, resulting in models that are more flexible and capable of learning post-training, with applications in various fields.
   - *Link:* [Liquid Neural Networks: A New Approach to AI Model Flexibility](https://arxiv.org/abs/2401.67890)

9. **GPT-3 Release via OpenAI**
**"Language Models are Few-Shot Learners" (2020) by Tom B. Brown et al.**  
   - *Summary:* Introduced GPT-3, a 175-billion parameter autoregressive language model capable of performing diverse natural language processing tasks without explicit fine-tuning, demonstrating strong performance in translation, question-answering, and more through few-shot learning.  
   - *Link:* [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)

10. **"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models" (2022) by Jason Wei et al.**  
   - *Summary:* Demonstrated that prompting large language models with a chain of thought—a series of intermediate reasoning steps—significantly enhances their ability to perform complex reasoning tasks, leading to improved performance in arithmetic, commonsense, and symbolic reasoning.  
   - *Link:* [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)

11. **"Self-Consistency Improves Chain of Thought Reasoning in Language Models" (2022) by Xuezhi Wang et al.**  
   - *Summary:* Proposed a new decoding strategy called self-consistency, which samples multiple reasoning paths and selects the most consistent answer, further enhancing the performance of chain-of-thought prompting in large language models across various reasoning benchmarks.  
   - *Link:* [Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/abs/2203.11171)
  
12. "ChatGPT: Optimizing Language Models for Dialogue," published on November 30, 2022
Summary: OpenAI introduced ChatGPT in November 2022, highlighting its conversational abilities, including answering follow-up questions, admitting mistakes, and rejecting inappropriate requests. It was trained using Reinforcement Learning from Human Feedback (RLHF), similar to InstructGPT. OpenAI launched ChatGPT as a research preview to gather user feedback on its strengths and limitations. The model demonstrates capabilities in coding assistance, general knowledge, and ethical content moderation

 13. **"Introducing ChatGPT" (2022) by OpenAI**  
   - *Summary:* Announced the release of ChatGPT, a conversational AI model fine-tuned using Reinforcement Learning from Human Feedback (RLHF), capable of handling follow-up questions, admitting mistakes, and rejecting inappropriate requests. Released as a research preview to gather feedback and improve capabilities.  
   - *Link:* [Introducing ChatGPT](https://openai.com/index/chatgpt/)



